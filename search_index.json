[["index.html", "Reinforcement Learning Chapter 1 Prerequisites", " Reinforcement Learning Renyi Qu 2022-12-02 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["model-based-rl.html", "Chapter 2 Model-based RL 2.1 Markov Decision Process (MDP)", " Chapter 2 Model-based RL 2.1 Markov Decision Process (MDP) Specification: State space: \\(\\mathcal{S}\\) Action space: \\(\\mathcal{A}\\) Transition probability (i.e., model): \\(p(s&#39;|s,a)=P(S_{t+1}=s&#39;|S_t=s,A_t=a)=\\sum_{r\\in\\mathcal{R}}p(s&#39;,r|s,a)\\) Reward: \\(r(s,a,s&#39;)=\\mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s&#39;]=\\frac{\\sum_{r\\in\\mathcal{R}}rp(s&#39;,r|s,a)}{\\sum_{r\\in\\mathcal{R}}p(s&#39;,r|s,a)}\\) Discount factor: \\(\\gamma\\in[0,1]\\) Goal: Find optimal policy \\(a_t=\\pi(s_t)\\) to maximize long-term reward: \\[ G_t=\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1} \\] Policy: Bellman’s Equation (\\(\\forall\\pi(a|s)\\), including stochastic) \\[\\begin{align*} V_\\pi(s)&amp;=E_\\pi[G_t|s_t=s]=\\sum_{a}\\pi(a|s)\\sum_{s&#39;,r}p(s&#39;,r|s,a)[r+\\gamma V_\\pi(s&#39;)], \\forall s\\in\\mathcal{S}\\\\ Q_\\pi(s,a)&amp;=E_\\pi[G_t|s_t=s,a_t=a]=\\sum_{s&#39;,r}p(s&#39;,r|s,a)[r+\\gamma V_\\pi(s&#39;)], \\forall s\\in\\mathcal{S}, \\forall a\\in\\mathcal{A} \\end{align*}\\] Bellman’s Optimality Equation (\\(\\forall\\pi_*(s)\\), only deterministic) \\[\\begin{align*} V_*(s)&amp;=\\max_{a\\in\\mathcal{A}(s)}\\sum_{s&#39;,r}p(s&#39;,r|s,a)[r+\\gamma V_*(s&#39;)], \\forall s\\in\\mathcal{S}\\\\ Q_*(s,a)&amp;=\\sum_{s&#39;,r}p(s&#39;,r|s,a)[r+\\gamma\\max_{a&#39;}Q_*(s&#39;)], \\forall s\\in\\mathcal{S}, \\forall a\\in\\mathcal{A} \\end{align*}\\] Dynamic Programming: Policy Evaluation: compute \\(V_\\pi\\) for input policy \\(\\pi\\) Init \\(V(s)\\in\\mathbb{R}; \\pi(s)\\in\\mathcal{A}(s); \\Delta=0; \\epsilon\\) Repeat until \\(\\Delta&lt;\\epsilon\\) (i.e., convergence: \\(\\{V_k\\}\\rightarrow V_\\pi\\) as \\(k\\rightarrow\\infty\\)): for \\(s\\in\\mathcal{S}\\): \\(V_\\text{prev}\\leftarrow V(s)\\) \\(V(s)\\leftarrow\\) Bellman’s Equation \\(\\Delta\\leftarrow\\max(\\Delta,|V_\\text{prev}-V(s)|)\\) Policy Improvement: update policy \\(\\pi\\) (each update guarantees a strictly better policy than before) Init policy_stable=True for \\(s\\in\\mathcal{S}\\): \\(a_\\text{prev}\\leftarrow\\pi(s)\\) \\(\\pi(s)\\rightarrow\\arg\\max_{a\\in\\mathcal{A}(s)}\\sum_{s&#39;,r}p(s&#39;,r|s,a)[r+\\gamma V(s)]\\) policy_stable=False if \\(a_\\text{prev}\\neq\\pi(s)\\) Policy Iteration: Policy Evaluation + Policy Improvement Repeat: Policy Evaluation Policy Improvement If policy_stable=True, return \\(\\pi\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
